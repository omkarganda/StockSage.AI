from __future__ import annotations

"""Generative Sentiment Features powered by LLMs

This module augments the traditional FinBERT-based sentiment analysis with
features generated by large language models (LLMs). Specifically, it can:

1. Summarise a set of news articles into a compact narrative summary.
2. Produce a holistic sentiment score using chain-of-thought reasoning.
3. Convert the LLM outputs into quantitative features that can be merged with
   the existing daily sentiment dataframe.

The functions are **best-effort** – if no LLM provider is configured the
functions will silently fall back to neutral values so that the overall data
pipeline keeps working.
"""

from typing import List, Optional

import pandas as pd

from ..utils.llm import safe_score_sentiment, safe_summarise_texts
from ..utils.logging import get_logger

logger = get_logger(__name__)

# ---------------------------------------------------------------------------
# Core helpers
# ---------------------------------------------------------------------------

def _compute_llm_sentiment_scores(texts: List[str]) -> List[float]:
    """Vectorised helper that calls the LLM once per text and returns scores."""
    scores: List[float] = []
    for txt in texts:
        try:
            score = safe_score_sentiment(txt)
        except Exception as exc:  # pragma: no cover – network errors etc.
            logger.warning("LLM sentiment scoring failed, defaulting to 0.0 – %s", exc)
            score = 0.0
        scores.append(score)
    return scores

# ---------------------------------------------------------------------------
# Public API
# ---------------------------------------------------------------------------

def add_llm_sentiment_features(
    articles_df: pd.DataFrame,
    text_columns: Optional[List[str]] = None,
    date_col: str = "publishedAt",
) -> pd.DataFrame:
    """Return *article-level* dataframe with additional LLM sentiment columns.

    The function expects a dataframe similar to what `NewsCollector.fetch_*`
    returns. It will create the following columns:

    * `llm_sentiment_score`  – float in [-1, 1]
    * `llm_summary`          – short summary (for inspection, not numeric)

    Notes
    -----
    • The function is deliberately conservative with API usage. For summaries we
      make **one** call per *day* (all articles concatenated) instead of per
      article so costs remain minimal.
    • If no LLM is available, the added columns will contain neutral defaults
      so downstream code does not need conditional logic.
    """

    if text_columns is None:
        text_columns = ["title", "description"]

    if articles_df.empty:
        logger.debug("Input articles dataframe empty – returning unchanged.")
        return articles_df

    # ------------------------------------------------------------------
    # 1. Article-level sentiment scores
    # ------------------------------------------------------------------
    concat_text = (
        articles_df[text_columns]
        .fillna("")
        .astype(str)
        .agg(" ".join, axis=1)
        .tolist()
    )
    articles_df["llm_sentiment_score"] = _compute_llm_sentiment_scores(concat_text)

    # ------------------------------------------------------------------
    # 2. Daily summary (one LLM call per unique date)
    # ------------------------------------------------------------------
    if date_col not in articles_df.columns:
        logger.warning("Column '%s' not found – cannot compute daily summaries", date_col)
        articles_df["llm_daily_summary"] = ""
        return articles_df

    articles_df[date_col] = pd.to_datetime(articles_df[date_col])
    articles_df["date_only"] = articles_df[date_col].dt.date

    summaries = {}
    for day, grp in articles_df.groupby("date_only"):
        texts_for_day = grp[text_columns].fillna("").astype(str).agg(" ".join, axis=1).tolist()
        try:
            summaries[day] = safe_summarise_texts(texts_for_day)
        except Exception as exc:  # pragma: no cover
            logger.warning("LLM summarisation failed for %s – %s", day, exc)
            summaries[day] = ""

    articles_df["llm_daily_summary"] = articles_df["date_only"].map(summaries).fillna("")

    return articles_df


def aggregate_llm_sentiment_daily(
    article_level_df: pd.DataFrame,
    date_col: str = "publishedAt",
    sentiment_col: str = "llm_sentiment_score",
) -> pd.DataFrame:
    """Aggregate article-level LLM sentiment to *daily* quantitative features."""
    if article_level_df.empty:
        return pd.DataFrame()

    df = article_level_df.copy()
    df[date_col] = pd.to_datetime(df[date_col])
    df["date_only"] = df[date_col].dt.date

    daily = (
        df.groupby("date_only")[sentiment_col]
        .agg(["mean", "median", "std", "count"])
        .rename(
            columns={
                "mean": "llm_sentiment_mean",
                "median": "llm_sentiment_median",
                "std": "llm_sentiment_std",
                "count": "llm_sentiment_article_count",
            }
        )
    )
    daily.index = pd.to_datetime(daily.index)

    return daily.reset_index().rename(columns={"date_only": "date"})